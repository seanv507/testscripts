---
title: "emulating Logistic regression with random forest/xgboost"
output: html_document
---

```{r}
library(randomForest)
library(xgboost)
logistic <- function(x) exp(x) / (1 + exp(x))
logloss <- function (target, pred) { -mean(ifelse(target > 0, log(pred), log(1-pred)))}
acc <- function(target,pred){ sum(target==pred)/length(target)}
```

```{r}
set.seed(1829)
```

```{r}
n <- 10000
x1 <- runif(n)
x2 <- runif(n)
inv_prob <- ( 1 * (x1 - x2))
y_prob <- logistic( inv_prob)
y <- rbinom(n, 1, y_prob)
dat <- data.frame(x1, x2, y)
```

## Split into training and test set
```{r}
train_frac <- .5
cases <- sample(seq_len(n), train_frac * n)
train <- dat[cases, ]
test <- dat[-cases, ]
```

## Build Logistic regression model
```{r}
# used to plot decision boundary
x2_lr <- function(mdl, x1){
  m <- coefficients(mod_lr)
  -(m[['(Intercept)']] + x1*m[['x1']])/m[['x2']]
}
mod_lr <- glm(y ~ x1 + x2 , binomial, train)
predtr_lr <- predict(mod_lr, train, type = "response")
pred_lr <- predict(mod_lr, test, type = "response")
coefficients(mod_lr)
test$lr_prob <- pred_lr
test$lr_class <- test$lr_prob > .5
acc_lr <- acc(test$y, test$lr_class)
acc_pop <- acc(test$y, (test$x1 - test$x2) > 0) # accuracy of true/population classifier
```

## Build random forest
```{r}
mod_rf <- randomForest(factor(y) ~ ., train, mtry=2, ntree=1000, nodesize=500)
pred_rf <- predict(mod_rf, test, type='prob')
test$rf <- pred_rf[,2] > 0.5
acc_rf <- acc(test$y, test$rf)
print( c(acc_lr, acc_rf, acc_pop))
```

```{r}
dtrain <- xgb.DMatrix(as.matrix(train[c('x1', 'x2')]), label = train$y)
dtest <- xgb.DMatrix(as.matrix(test[c('x1', 'x2')]), label = test$y)
xgb_params = list(
  eta = 0.1,
  subsample=0.7,
  objective = "binary:logistic",
  max_depth=5,
  eval.metric = "logloss"
)

cv <- xgb.cv(xgb_params, data=dtrain, nrounds = 100, nthread = 2, nfold = 5,
            early_stopping_rounds=15,
            watchlist=list(train=dtrain))
best_nrounds = cv$best_iteration
cv_mean = cv$evaluation_log$test_error_mean[best_nrounds]
cv_std = cv$evaluation_log$test_error_std[best_nrounds]
cat(paste0('CV-Mean: ',cv_mean,' ', cv_std))

bst = xgb.train(xgb_params, dtrain, best_nrounds)              
bst <- xgb.train(xgb_params, data = dtrain, nrounds=best_nrounds, 
                 watchlist=list(train=dtrain,test=dtest))

predtr_bst <- predict(bst, as.matrix(train[c('x1', 'x2')]))
pred_bst <- predict(bst, as.matrix(test[c('x1', 'x2')]))
acc_bst <- acc(test$y, pred_bst > 0.5)
print( c(acc_pop, acc_lr, acc_rf, acc_bst))
log_lr <- logloss(test$y, pred_lr)
#log_rf <- logloss(test$y, ifelse(pred_rf<1e-8, 1e-8, ifelse(pred_rf< 1 - 1e-8, pred_rf,  1 - 1e-8))) uncalibrated, meaningless
log_bst <- logloss(test$y, pred_bst)
print( c(log_lr, log_bst))
```


classes.train <- ifelse(train$x1>train$x2, "blue", "orange")
classes.test <- ifelse(test$x1>test$x2, "blue", "orange")
```{r}
ran <- seq(0,1,0.01)
grid <- expand.grid(x1=ran, x2=ran)
classes.grid <- predict(mod_rf, grid) ==1
prob_lr.grid <- predict(mod_lr, grid, type='response')
prob_rf.grid <- predict(mod_rf, grid, type='prob')[,2]
contour_levels <- seq(min(prob_lr.grid), max(prob_lr.grid), length=10)
contour_levels_rf <- seq(min(prob_rf.grid), max(prob_rf.grid), length=10)
prob_bst.grid <- predict(bst, as.matrix(grid))
# plot the boundary
# contour(x=ran, y=ran, z=matrix(classes.grid, nrow=length(ran)), levels=0.5,
#        col="grey", drawlabels=FALSE, lwd=2)
filled.contour(x=ran, y=ran, z=matrix(prob_rf.grid, nrow=length(ran)), levels=contour_levels_rf)
title('Random Forest: uncalibrated')
filled.contour(x=ran, y=ran, z=matrix(prob_bst.grid, nrow=length(ran)), levels=contour_levels)
title('xgboost')
filled.contour(x=ran, y=ran, z=matrix(prob_lr.grid, nrow=length(ran)), levels=contour_levels)
title('logistic regression')
```
# add points from test dataset
points(test[c('x1','x2')], col=test$y*3 + 1)
#points(train[c('x1','x2')], col=train$y*3 + 2)
lines(c(0,1), c(0,1),col=10)
lines(c(0,1), x2_lr(mod_lr,c(0,1)), col=5)


plot(test$x1,test$x2, col=1 + test$lr_class)

title(paste('logistic regression ',paste(mod_lr$coefficients)))

plot(test$x1,test$x2, col=5 + test$y)
lines(c(0,1), c(0,1),col=1)
title('test data')

plot(test$x1,test$x2, col=3 + test$rf)
lines(c(0,1), c(0,1),col=1)
title('random forest')
cuts <- seq(0,1,.1) +0.05
midpoints <- seq(0,.9,.1) +0.05
test$x1c <- cut(test$x1,cuts)
test$x2c <- cut(test$x2,cuts)


